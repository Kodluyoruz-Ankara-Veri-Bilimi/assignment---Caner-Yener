{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desktop = os.path.join(os.path.join(os.environ['USERPROFILE']), 'Desktop') \n",
    "\n",
    "os.chdir(desktop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataInfo():\n",
    "    def __init__(self, filename):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import os\n",
    "        if filename[-4:] == \".xls\" or filename[-5:] == \".xlsx\":\n",
    "            self.data = pd.read_excel(filename)\n",
    "        elif filename[-4:] == \".csv\":\n",
    "            self.data = pd.read_csv(filename)\n",
    "\n",
    "    def dfhead(self, rows=None):\n",
    "        #veri tablosunun ilk 5 satırı\n",
    "        if rows != None:\n",
    "            return self.data.head(rows)\n",
    "        return self.data.head()\n",
    "               \n",
    "    def null(self):\n",
    "        null = self.data.isnull()\n",
    "        return null\n",
    "    \n",
    "    def dfdescribe(self):\n",
    "        describe = self.data.describe().T\n",
    "        return describe\n",
    "    \n",
    "    def dfinfo(self):\n",
    "        info = self.data.info()\n",
    "        return info \n",
    "    \n",
    "    def dfdropna(self):\n",
    "        return self.data.dropna()\n",
    "    \n",
    "    def missing(self):\n",
    "        missing = self.data.isnull().sum()\n",
    "        return missing\n",
    "    \n",
    "    def countval(self, col):\n",
    "        counts = self.data[col].value_counts()\n",
    "        return counts\n",
    "    \n",
    "\n",
    "class Visualize():\n",
    "    def __init__(self, data):\n",
    "        import seaborn as sns\n",
    "        import matplotlib.pyplot as plt\n",
    "        self.data = data\n",
    "    \n",
    "    def hist(self):\n",
    "        import seaborn as sns\n",
    "        import matplotlib.pyplot as plt\n",
    "        df = self.data\n",
    "        numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"])\n",
    "        #fig, ax = plt.subplots(len(numeric_cols.columns), 1, figsize = (10, 30))\n",
    "        #print(numeric_cols.iloc[:,0])\n",
    "        for i in range (len(numeric_cols.columns)):\n",
    "            plt.hist(numeric_cols.iloc[:,i], bins=40)\n",
    "            #ax[i].hist(numeric_cols.iloc[:,i], bins=40)\n",
    "            plt.title(numeric_cols.columns[i])\n",
    "            plt.show()\n",
    "            \n",
    "    \n",
    "    def boxplot(self):\n",
    "        import seaborn as sns\n",
    "        import matplotlib.pyplot as plt\n",
    "        df = self.data\n",
    "        numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"])\n",
    "        numeric_cols.boxplot(figsize=(20,15))\n",
    "        \n",
    "    \n",
    "    def countplot(self):\n",
    "        import seaborn as sns\n",
    "        import matplotlib.pyplot as plt\n",
    "        df = self.data\n",
    "        categoric_cols = df.select_dtypes(exclude=[\"int64\", \"float64\"])\n",
    "        fig, ax = plt.subplots(len(categoric_cols.columns),1, figsize=(10,7))\n",
    "        for i in range (len(categoric_cols.columns)):\n",
    "            sns.countplot(x=categoric_cols.iloc[:,i],palette='pastel', ax = ax[i])\n",
    "            \n",
    "    def correlation(self):\n",
    "        import seaborn as sns\n",
    "        import matplotlib.pyplot as plt\n",
    "        df = self.data\n",
    "        fig,ax = plt.subplots(figsize=(30, 30))\n",
    "        sns.heatmap(df.corr(), ax=ax, annot=True, \n",
    "            linewidths=0.05, fmt= '.2f',cmap=\"summer_r\")\n",
    "        plt.show()\n",
    "        \n",
    "    def jointplot(self):\n",
    "        import seaborn as sns\n",
    "        import matplotlib.pyplot as plt\n",
    "        df = self.data\n",
    "        numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"])\n",
    "        #fig, ax = plt.subplots(len(numeric_cols.columns),1, figsize=(10,7))\n",
    "        for i in range (len(numeric_cols.columns)):\n",
    "            for j in range(len(numeric_cols.columns)):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                sns.jointplot(x=numeric_cols.iloc[:,i],y=numeric_cols.iloc[:,j], data=df, kind=\"reg\")\n",
    "        #plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Models():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def split(self, targetcol, to_drop = []):\n",
    "        df = self.data\n",
    "        \n",
    "        self.y = df[targetcol]\n",
    "        self.X = df.drop(to_drop,axis=1)\n",
    "        self.X = pd.DataFrame(self.X)\n",
    "         \n",
    "        X_train,X_test,y_train,y_test = train_test_split(self.X,self.y,test_size=0.30,random_state=42)\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "    \n",
    "    \n",
    "    def LogReg(self):\n",
    "        loj = sm.Logit(self.y_train,self.X_train)\n",
    "        loj_model = loj.fit()\n",
    "        summary = loj_model.summary()\n",
    "    \n",
    "        loj2 = LogisticRegression(solver = \"liblinear\")\n",
    "        logreg = loj2.fit(self.X, self.y)\n",
    "        intercept = logreg.intercept_\n",
    "        coefs = logreg.coef_\n",
    "    \n",
    "        print (f\"OLS: {summary}\")\n",
    "        print (f\"SKLearn LogReg: {logreg}\")\n",
    "        print (f\"LogReg Intercept: {intercept}\")\n",
    "        print (f\"LogReg Coefficients: {coefs}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def LogRegPrediction(self):\n",
    "    \n",
    "        loj = LogisticRegression(solver = \"liblinear\") \n",
    "        lojmodel = loj.fit(self.X_train,self.y_train)\n",
    "    \n",
    "        y_pred = lojmodel.predict(self.X_test)\n",
    "    \n",
    "        accuracy = accuracy_score(self.y_test,y_pred)\n",
    "        cm = confusion_matrix(self.y_test,y_pred)\n",
    "        classreport = classification_report(self.y_test,y_pred)\n",
    "    \n",
    "        logit_roc_auc = roc_auc_score(self.y_train,lojmodel.predict(self.X_train))\n",
    "        fpr, tpr, thresholds = roc_curve(self.y_train,lojmodel.predict_proba(self.X_train)[:,1])\n",
    "\n",
    "        ROC = plt.figure(),\n",
    "        plt.plot(fpr, tpr, label = \"AUC (area = %0.2f)\"% logit_roc_auc),\n",
    "        plt.plot([0,1], [0,1], \"r--\"),\n",
    "        plt.xlim([0.0, 1.0]),\n",
    "        plt.ylim([0.0,1.05]),plt.xlabel(\"FP Oranı\"),\n",
    "        plt.ylabel(\"TP Oranı\"),\n",
    "        plt.title(\"ROC\")\n",
    "    \n",
    "    \n",
    "        cross_val_train = cross_val_score(lojmodel, self.X_train, self.y_train, cv = 10).mean()\n",
    "        cross_val_test = cross_val_score(lojmodel, self.X_test, self.y_test, cv = 10).mean()\n",
    "\n",
    "    \n",
    "    \n",
    "        print(f\"Accuracy : {accuracy}\")\n",
    "        print(\"-------------------\")\n",
    "        print(f\"Confusion Matrix: \\n {cm}\")\n",
    "        print(\"-------------------\")\n",
    "        print(f\"Classification report: \\n {classreport}\")\n",
    "        print(\"-------------------\")\n",
    "        print(f\"\"\"Cross validation accuracy values: \n",
    "              training: {cross_val_train}\n",
    "              test: {cross_val_test}\"\"\")\n",
    "        print(\"-------------------\")\n",
    "        print(ROC)\n",
    "\n",
    "    def pca(self):\n",
    "        pca = PCA()\n",
    "        self.X_reduced_train = pca.fit_transform(scale(self.X_train))\n",
    "        cumulativesum = np.cumsum(np.round(pca.explained_variance_ratio_, decimals = 4)*100)[:]\n",
    "        components = range(pca.n_components_)\n",
    "    \n",
    "        figure = plt.figure(figsize=(15,10)),\n",
    "        plt.bar(components,pca.explained_variance_ratio_,color = \"red\"),\n",
    "        plt.xlim(0,25),\n",
    "        plt.xlabel(\"PCA Bilesenler\"),\n",
    "        plt.ylabel(\"Aciklanan Varyans %\"),\n",
    "        plt.xticks(components)\n",
    "        plt.show()\n",
    "    \n",
    "        lm = LinearRegression()\n",
    "        pca_model = lm.fit(self.X_reduced_train[:,:], self.y_train)  \n",
    "\n",
    "        ols = sm.OLS(self.y_train,self.X_reduced_train[:,:])\n",
    "        pca_model2  = ols.fit()\n",
    "    \n",
    "        y_pred = pca_model.predict(self.X_reduced_train[:,:])\n",
    "        rmse1 = np.sqrt(mean_squared_error(self.y_train,y_pred))\n",
    "        \n",
    "        PCA_components = pd.DataFrame(self.X_reduced_train)\n",
    "        PCAPlot = plt.scatter(PCA_components[0] , PCA_components[1], alpha = .1 , color='black'),\n",
    "        plt.xlabel('PCA 1'),\n",
    "        plt.ylabel('PCA 2');\n",
    "        \n",
    "        print(f\"Bilesen sayisina gore aciklanabilirlik: \\n {cumulativesum}\")\n",
    "        print(f\"Açıklayıcılık grafiği: \\n {figure}\")\n",
    "        print(f\"Intercept: \\n {pca_model.intercept_}\")\n",
    "        print(f\"Coefs: \\n {pca_model.coef_}\")\n",
    "        print(f\"OLS tablosu : \\n {pca_model2.summary().tables[0]}\")\n",
    "        print(f\"PCA sonrası RMSE değeri: {rmse1}\")\n",
    "        print(PCAPlot)\n",
    "     \n",
    "\n",
    "    def nnclassifier(self):\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scale = scaler.fit_transform(self.X_train)\n",
    "        X_test_scale = scaler.fit_transform(self.X_test)\n",
    "\n",
    "        mlp_regres = MLPClassifier().fit(X_train_scale,self.y_train) \n",
    "        y_pred = mlp_regres.predict(X_test_scale)\n",
    "        accuracy = accuracy_score(self.y_test,y_pred)\n",
    "        cm = confusion_matrix(self.y_test,y_pred)\n",
    "\n",
    "        mlp_params = {\"alpha\":[0.1,0.01,0.02,0.005], \n",
    "              \"hidden_layer_sizes\":[(20,20),(100,50,150),(300,200,150)],\n",
    "              'solver': ['adam', 'lbfgs']} \n",
    "\n",
    "        mlp_c = MLPClassifier()\n",
    "        mlp_c_model = GridSearchCV(mlp_c,mlp_params,\n",
    "                       cv = 10,\n",
    "                       n_jobs = -1,\n",
    "                       verbose = 2)\n",
    "\n",
    "        mlp_tuned = MLPClassifier(alpha=list(mlp_c_model.best_params_.values())[0],\n",
    "                                  hidden_layer_sizes=list(mlp_c_model.best_params_.values())[1],\n",
    "                         solver=list(mlp_c_model.best_params_.values())[2]).fit(X_train_scale,y_train)\n",
    "\n",
    "        y_pred = mlp_tuned.predict(X_test_scale)\n",
    "\n",
    "        accuracy_tuned = accuracy_score(self.y_test,y_pred)\n",
    "        cm_tuned = confusion_matrix(self.y_test,y_pred)\n",
    "\n",
    "\n",
    "        print(\"Accuracy Before Tuning: \",accuracy)\n",
    "        print(\"CM before tuning: \\n\",cm)\n",
    "        print(\"Accuracy tuned:\",accuracy_tuned)\n",
    "        print(\"CM tuned: \\n\",cm_tuned)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def nnregressor(self):\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scale = scaler.fit_transform(self.X_train)\n",
    "        X_test_scale =  scaler.fit_transform(self.X_test)\n",
    "  \n",
    "        mlp_reg = MLPRegressor()\n",
    "        mlp_reg_model = mlp_reg.fit(X_train_scale,self.y_train) \n",
    "        y_pred = mlp_reg_model.predict(X_test_scale)\n",
    "        rmse = np.sqrt(mean_squared_error(self.y_test,y_pred))\n",
    "\n",
    "        mlp_params = {\"alpha\":[0.1,0.01,0.02,0.005], \n",
    "             \"hidden_layer_sizes\":[(20,20),(100,50,150),(300,200,150)],\n",
    "              'solver': ['adam', 'lbfgs'],\n",
    "              'learning_rate': ['constant','adaptive']} \n",
    "\n",
    "        mlp_cv_model = GridSearchCV(mlp_reg,mlp_params,cv = 10)\n",
    "        mlp_cv_model = mlp_cv_model.fit(X_train_scale,self.y_train)\n",
    "        mlp_tuned = MLPRegressor(\n",
    "                         alpha=list(mlp_cv_model.best_params_.values())[0],\n",
    "                        hidden_layer_sizes=list(mlp_cv_model.best_params_.values())[1],\n",
    "                         learning_rate=list(mlp_cv_model.best_params_.values())[2],\n",
    "                         solver=list(mlp_cv_model.best_params_.values())[3]).fit(X_train_scale,y_train)\n",
    "\n",
    "        y_pred = mlp_tuned.predict(X_test_scale)\n",
    "\n",
    "        rmse_tuned = np.sqrt(mean_squared_error(self.y_test,y_pred))\n",
    "\n",
    "        print(\"RMSE before tuning: \",rmse)\n",
    "        print(\"RMSE after tuning:\",rmse_tuned)\n",
    "        \n",
    "        \n",
    "    def dectreeregressor(self):\n",
    "        \n",
    "        cart = DecisionTreeRegressor()\n",
    "        cart_model = cart.fit(self.X_train,self.y_train)\n",
    "\n",
    "        y_pred = cart_model.predict(self.X_test)\n",
    "        rmse1 = np.sqrt(mean_squared_error(self.y_test, y_pred))\n",
    "\n",
    "        cart_grid = {\"max_depth\": range(1,10),\n",
    "            \"min_samples_split\" : list(range(2,50))}\n",
    "\n",
    "        cart_cv_model = GridSearchCV(cart_model, cart_grid, cv = 10, n_jobs=-1, verbose=2)\n",
    "\n",
    "        cart_cv_model.fit(self.X_train, self.y_train)\n",
    "\n",
    "        cart_tuned = DecisionTreeRegressor(max_leaf_nodes = list(cart_cv_model.best_params_.values())[0],\n",
    "                                           min_samples_split = list(cart_cv_model.best_params_.values())[1])\n",
    "        cart_tuned.fit(self.X_train, self.y_train)\n",
    "        y_pred = cart_tuned.predict(self.X_test)\n",
    "        rmse_tuned = np.sqrt(mean_squared_error(self.y_test, y_pred)) \n",
    "\n",
    "\n",
    "        print(f\"RMSE (ilk değer): \\n {rmse1}\\n RMSE tuned: \\n {rmse_tuned}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    def dectreeclassifier(self):\n",
    "        cart = DecisionTreeClassifier()\n",
    "        cart_model = cart.fit(self.X_train, self.y_train)\n",
    "        y_pred = cart_model.predict(self.X_test)\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        cm = confusion_matrix(self.y_test,y_pred)\n",
    "        \n",
    "        print(f\"Decision tree classifier accuracy: {accuracy}\")\n",
    "        print(f\"Decision Tree Confusion Matrix: \\n {cm}\")\n",
    "\n",
    "        cart_grid = {\"max_depth\": range(1,10),\n",
    "            \"min_samples_split\" : list(range(2,50))}\n",
    "\n",
    "        cart = DecisionTreeClassifier()\n",
    "        cart_cv = GridSearchCV(cart,cart_grid, cv = 10,n_jobs=-1,verbose=2)\n",
    "        cart_cv_model = cart_cv.fit(self.X_train, self.y_train)\n",
    "\n",
    "        bos = []\n",
    "\n",
    "        for i in cart_cv_model.best_params_:\n",
    "              bos.append(cart_cv_model.best_params_[i])\n",
    "\n",
    "        cart = DecisionTreeClassifier(max_depth = list(cart_cv_model.best_params_.values())[0],\n",
    "                                      min_samples_split = list(cart_cv_model.best_params_.values())[1])\n",
    "        cart_tuned = cart.fit(self.X_train, self.y_train)\n",
    "\n",
    "        y_pred = cart_tuned.predict(self.X_test)\n",
    "        accuracy_tuned = accuracy_score(self.y_test, y_pred)\n",
    "        cm_tuned = confusion_matrix(self.y_test,y_pred)\n",
    "\n",
    "        print(\"Tuned accuracy ve confusion matrix: \")\n",
    "        print(accuracy_tuned)\n",
    "        print(cm_tuned) \n",
    "\n",
    "    def rfclassifier(self):\n",
    "        rf_model = RandomForestClassifier().fit(self.X_train, self.y_train)\n",
    "        y_pred = rf_model.predict(self.X_test)\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)  \n",
    "        cm = confusion_matrix(self.y_test,y_pred)\n",
    "\n",
    "        rf_params = {\"max_depth\": [2,5,8,10],\n",
    "            \"max_features\": [2,5,8],\n",
    "            \"n_estimators\": [10,500,1000],\n",
    "            \"min_samples_split\": [2,5,10]}\n",
    "\n",
    "\n",
    "        rf_model = RandomForestClassifier()\n",
    "        rf_cv_model = GridSearchCV(rf_model, rf_params, cv = 10,\n",
    "                           n_jobs = -1,\n",
    "                           verbose = 2)\n",
    "\n",
    "        rf_cv_model.fit(self.X_train,self.y_train)  \n",
    "\n",
    "        rf_tuned = RandomForestClassifier(max_depth=list(rf_cv_model.best_params_.values())[0],\n",
    "                                            max_features=list(rf_cv_model.best_params_.values())[1],\n",
    "                                            min_samples_split=list(rf_cv_model.best_params_.values())[2],\n",
    "                                            n_estimators=list(rf_cv_model.best_params_.values())[3])\n",
    "\n",
    "        rf_tuned = rf_tuned.fit(self.X_train,self.y_train)\n",
    "        y_pred = rf_tuned.predict(self.X_test)\n",
    "\n",
    "        accuracy_tuned = accuracy_score(self.y_test,y_pred)\n",
    "        cm_tuned = confusion_matrix(self.y_test,y_pred)\n",
    "\n",
    "\n",
    "        Importance = pd.DataFrame({\"Importance\": rf_tuned.feature_importances_*100},\n",
    "                         index = self.X_train.columns)\n",
    "        importance = Importance.sort_values(by = \"Importance\", \n",
    "                       axis = 0, \n",
    "                       ascending = True).plot(kind =\"barh\", color = \"yellow\"),plt.xlabel(\"Değişken Önem Düzeyleri\")\n",
    "\n",
    "        print(f\"En iyi parametre degerleri: {rf_cv_model.best_params_}\")\n",
    "        print(\"                                               \")\n",
    "        print(\"Tune oncesi Accuracy ve Confusion Matrix degerleri: \",accuracy,\"\\n\",cm)\n",
    "        print(\"Tune sonrasi Accuracy ve Confusion Matrix degerleri: \",accuracy_tuned,\"\\n\",cm_tuned)\n",
    "        print(\"                                                \")\n",
    "        print(importance)\n",
    "\n",
    "    def rfregressor(self):\n",
    "        rf_model = RandomForestRegressor()\n",
    "        rf_model.fit(self.X_train, self.y_train)\n",
    "        y_pred = rf_model.predict(self.X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(self.y_test, y_pred))\n",
    "\n",
    "        rf_params = {'max_depth': list(range(1,10)),\n",
    "            'max_features': [3,5,10,15],\n",
    "            'n_estimators' : [100, 200, 500, 1000, 2000],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'min_samples_split': [2, 5, 10]}\n",
    "\n",
    "        rf_model = RandomForestRegressor()\n",
    "\n",
    "        rf_cv_model = GridSearchCV(rf_model, \n",
    "                           rf_params, \n",
    "                           cv = 10, \n",
    "                            n_jobs = -1,verbose = 2)\n",
    "\n",
    "        rf_cv_model.fit(X_train, y_train)\n",
    "\n",
    "        rf_tuned = RandomForestRegressor(max_depth  = list(rf_cv_model.best_params_.values())[0], \n",
    "                                 max_features = list(rf_cv_model.best_params_.values())[1], \n",
    "                                 min_samples_leaf = list(rf_cv_model.best_params_.values())[2],\n",
    "                                 min_samples_split = list(rf_cv_model.best_params_.values())[3],\n",
    "                                 n_estimators = list(rf_cv_model.best_params_.values())[4])\n",
    "\n",
    "        rf_tuned.fit(self.X_train, self.y_train)\n",
    "        y_pred = rf_tuned.predict(self.X_test)\n",
    "        rmse_tuned = np.sqrt(mean_squared_error(self.y_test, y_pred))\n",
    "\n",
    "        Importance = pd.DataFrame({\"Importance\": rf_tuned.feature_importances_*100},\n",
    "                         index = self.X_train.columns)\n",
    "        importance = Importance.sort_values(by = \"Importance\", \n",
    "                       axis = 0, \n",
    "                       ascending = True).plot(kind =\"barh\", color = \"r\"),plt.xlabel(\"Değişken Önem Düzeyleri\")\n",
    "\n",
    "        print(\"En iyi parametre degerleri: \",rf_cv_model.best_params_)\n",
    "        print(\"                                               \")\n",
    "        print(\"RMSE before tuning: \",rmse)\n",
    "        print(\"RMSE tuned: \",rmse_tuned)\n",
    "        print(\"                                                \")\n",
    "        print(importance)\n",
    "\n",
    "    def svmclass(self):\n",
    "        \n",
    "        svm_model = SVC(kernel=\"linear\").fit(self.X_train, self.y_train)\n",
    "        y_pred = svm_model.predict(self.X_test)\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        class_report = classification_report(self.y_test, y_pred)\n",
    "\n",
    "        svc_params= {\"C\": list(np.arange(1,10,1))}\n",
    "        svc = SVC()\n",
    "        \n",
    "        svc_cv_model = GridSearchCV(svc, svc_params, \n",
    "                         cv = 10, \n",
    "                         n_jobs = -1,\n",
    "                         verbose = 2)\n",
    "        \n",
    "        svc_tuned = SVC(C = list(svc_cv_model.best_params_.values())[0]).fit(self.X_train, self.y_train)\n",
    "    \n",
    "        y_pred = svc_tuned.predict(self.X_test)\n",
    "        accuracy_tuned = accuracy_score(self.y_test, y_pred)\n",
    "        class_report_tuned= classification_report(self.y_test,y_pred)\n",
    "    \n",
    "        print(\"Sınıflandırma raporu :\\n\",class_report_tuned)\n",
    "\n",
    "    def svmregressor(self):\n",
    "        svr = SVR(\"linear\")\n",
    "        svr_model = svr.fit(self.X_train, self.y_train)\n",
    "        y_pred = svr_model.predict(self.X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(self.y_test, y_pred))\n",
    "        \n",
    "        svr_params = {\"C\": np.arange(1,10,1)}\n",
    "        svr_cv_model = GridSearchCV(svr, svr_params, cv = 10,n_jobs=-1,verbose=2).fit(X_train,y_train)\n",
    "        svr_tuned = SVR(\"linear\", \n",
    "                C = pd.Series(svr_cv_model.best_params_)[0]).fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "        y_pred = svr_tuned.predict(X_test)\n",
    "        rmse_tuned = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "        print(\"RMSE before tuning: \\n\",rmse)\n",
    "        print(\"RMSE after tuning: \",rmse_tuned)\n",
    "        \n",
    "\n",
    "    def naivebayes(self):\n",
    "        \n",
    "        nb = GaussianNB()\n",
    "        nb_model = nb.fit(self.X_train,self.y_train)\n",
    "    \n",
    "        nb_y_pred = nb_model.predict(self.X_test)\n",
    "        nb_accuracy = accuracy_score(self.y_test,nb_y_pred)\n",
    "    \n",
    "        mnb = MultinomialNB()\n",
    "        mnb_model = mnb.fit(self.X_train,self.y_train)\n",
    "        \n",
    "        mnb_y_pred = model.predict(self.X_test)\n",
    "        mnb_accuracy = accuracy_score(self.y_test,mnb_y_pred)\n",
    "    \n",
    "        bnb = BernoulliNB()\n",
    "        bernoulli_model = bnb.fit(self.X_train,self.y_train)\n",
    "    \n",
    "        bnb_y_pred = bernoulli_model.predict(self.X_test)\n",
    "        bnb_accuracy = accuracy_score(self.y_test,bnb_y_pred)\n",
    "    \n",
    "        print(\"GaussianNB Accuracy: \", nb_accuracy)\n",
    "        print(\"MultinomialNB accuracy: \", mnb_accuracy)\n",
    "        print(\"BernoulliNB accuracy: \", bnb_accuracy)\n",
    "        print(\"GaussianNB crossval: \", cross_val_score(nb_model, self.X_test, self.y_test, cv=10).mean())\n",
    "        print(\"MultinomialNB crossval: \", cross_val_score(mnb_model, self.X_test, self.y_test, cv=10).mean())\n",
    "        print(\"BernoulliNB crossval: \", cross_val_score(bnb_model, self.X_test, self.y_test, cv=10).mean())\n",
    "\n",
    "\n",
    "    def lgbm_regressor(self):\n",
    "        lgbm = LGBMRegressor()\n",
    "        lgbm_model = lgbm.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        y_pred = lgbm_model.predict(self.X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(self.y_test, y_pred))\n",
    "        \n",
    "        lgbm_grid = {\n",
    "        'colsample_bytree': [0.4, 0.5,0.6,0.9,1],\n",
    "        'learning_rate': [0.01, 0.1, 0.3,0.5,1],\n",
    "        'n_estimators': [100,500,1000,2000],\n",
    "        'max_depth': [1,2,3,4,5,6,7,8],\n",
    "        \"subsample\": [0.1,0.3,0.5,0.8],\n",
    "        \"min_child_samples\":[50,100,200]}\n",
    "    \n",
    "    \n",
    "        lgbm = LGBMRegressor()\n",
    "        lgbm_cv_model = GridSearchCV(lgbm, lgbm_grid, cv=10, n_jobs = -1, verbose = 2)\n",
    "        lgbm_cv_model.fit(self.X_train, self.y_train)\n",
    "    \n",
    "        \n",
    "        lgbm_tuned = LGBMRegressor(learning_rate = list(lgbm_cv_model.best_params_.values())[1], \n",
    "                           max_depth = list(lgbm_cv_model.best_params_.values())[2], \n",
    "                           n_estimators = list(lgbm_cv_model.best_params_.values())[4],\n",
    "                          colsample_bytree = list(lgbm_cv_model.best_params_.values())[0],\n",
    "                          subsample=list(lgbm_cv_model.best_params_.values())[5],\n",
    "                          min_child_samples=list(lgbm_cv_model.best_params_.values())[3])\n",
    "\n",
    "        lgbm_tuned = lgbm_tuned.fit(self.X_train,self.y_train)\n",
    "    \n",
    "        y_pred = lgbm_tuned.predict(self.X_test)\n",
    "        rmse_tuned = np.sqrt(mean_squared_error(self.y_test, y_pred))\n",
    "    \n",
    "        print(\"Best parametreler: \",lgbm_cv_model.best_params_)\n",
    "        print(\"RMSE before tuning: \",rmse)\n",
    "        print(\"RMSE tuned: \",rmse_tuned)\n",
    "        \n",
    "    \n",
    "    def lgbm_class(self):\n",
    "        lgbm = LGBMClassifier()\n",
    "        lgbm_model = lgbm.fit(self.X_train,self.y_train)\n",
    "\n",
    "        y_pred = lgbm_model.predict(self.X_test)\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        \n",
    "        lgbm_params = {\n",
    "        'n_estimators': [100, 500, 1000],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5, 6],\n",
    "        'learning_rate': [0.001,0.01,0.1,0.2,0.3],\n",
    "        \"min_child_samples\": [20,50,75,100]}\n",
    "\n",
    "        lgbm_cv_model = GridSearchCV(lgbm, lgbm_params, \n",
    "                             cv = 10, \n",
    "                             n_jobs = -1, \n",
    "                             verbose = 2)\n",
    "\n",
    "        lgbm_cv_model.fit(self.X_train, self.y_train)\n",
    "\n",
    "        lgbm = LGBMClassifier(learning_rate = list(lgbm_cv_model.best_params_.values())[0], \n",
    "                       max_depth = list(lgbm_cv_model.best_params_.values())[1],\n",
    "                       subsample = list(lgbm_cv_model.best_params_.values())[4],\n",
    "                       n_estimators = list(lgbm_cv_model.best_params_.values())[3],\n",
    "                       min_child_samples = list(lgbm_cv_model.best_params_.values())[2])\n",
    "    \n",
    "        lgbm_tuned = lgbm.fit(self.X_train,self.y_train)\n",
    "\n",
    "        y_pred = lgbm_tuned.predict(self.X_test)\n",
    "        accuracy_tuned = accuracy_score(self.y_test, y_pred)\n",
    "        class_report_tuned = classification_report(self.y_test,y_pred)\n",
    "    \n",
    "    \n",
    "        print(\"Best parametreler: \",lgbm_cv_model.best_params_)\n",
    "        print(\"Accuracy tuned: \",accuracy)\n",
    "        print(\"Tuned sınıflandırma raporu: \\n\",class_report_tuned)\n",
    "  \n",
    "    def knnclassifier(self):\n",
    "        knn = KNeighborsClassifier()\n",
    "        knn_model = knn.fit(self.X_train, self.y_train)\n",
    "        knn_model\n",
    "        \n",
    "        y_pred = knn_model.predict(self.X_test)\n",
    "\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "\n",
    "        cr = classification_report(self.y_test, y_pred)\n",
    "        \n",
    "        knn_params = {\"n_neighbors\": np.arange(1,50),\n",
    "             \"metric\": [\"minkowski\", \"euclidean\", \"manhattan\", \"hamming\"]}\n",
    "        \n",
    "        knn_cv = GridSearchCV(knn,knn_params, cv=10, n_jobs=-1, verbose=2)\n",
    "        knn_cv.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors=list(knn_cv.best_params_.values())[0], \n",
    "                                   metric = list(knn_cv.best_params_.values())[1])\n",
    "        knn_tuned = knn.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        y_pred = knn_model.predict(self.X_test)\n",
    "\n",
    "        accuracy_tuned = accuracy_score(self.y_test, y_pred)\n",
    "\n",
    "        cr_tuned = classification_report(self.y_test, y_pred)\n",
    "\n",
    "\n",
    "        print(\"Best parametreler: \\n\",knn_cv.best_params_)\n",
    "        print(\"KNN class accuracy: \",accuracy)\n",
    "        print(\"KNN class classification report: \\n\",cr)\n",
    "        print(\"KNN class accuracy tuned: \",accuracy_tuned)\n",
    "        print(\"KNN class classification report tuned: \\n\",cr_tuned)\n",
    "        \n",
    "    def knnregressor(self):\n",
    "        knnr = KNeighborsRegressor()\n",
    "        knn_model = knnr.fit(self.X_train, self.y_train)\n",
    "        knn_model\n",
    "        \n",
    "        y_pred = knn_model.predict(self.X_test)\n",
    "        rmse=np.sqrt(mean_squared_error(self.y_test, y_pred))\n",
    "        \n",
    "        knn_params = {\"n_neighbors\": np.arange(1,30),\n",
    "             \"weights\": [\"uniform\", \"distance\"],\n",
    "             \"metric\": [\"minkowski\", \"euclidean\", \"manhattan\", \"hamming\"]}\n",
    "        \n",
    "        knn_cv = GridSearchCV(knnr, knn_params, cv=10, n_jobs=-1, verbose=2)\n",
    "        knn_cv.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        knnr = KNeighborsClassifier(n_neighbors=list(knn_cv.best_params_.values())[0], \n",
    "                                   metric = list(knn_cv.best_params_.values())[1])\n",
    "        knn_tuned = knnr.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        y_pred = knn_model.predict(self.X_test)\n",
    "\n",
    "        rmse_tuned = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "\n",
    "        print(\"Best parametreler: \\n\",knn_cv.best_params_)\n",
    "        print(\"KNN class RMSE: \",rmse)\n",
    "        print(\"KNN class RMSE tuned: \",rmse_tuned)\n",
    "        \n",
    "\n",
    "    def kelbow(self):\n",
    "        df = self.data\n",
    "        kmeans = KMeans()\n",
    "        visualizer = KElbowVisualizer(kmeans, k=(1,15))\n",
    "        visualizer.fit(df) \n",
    "        visualizer.poof()\n",
    "        \n",
    "    def cluster(self):\n",
    "        df = self.X\n",
    "        kmeans = KMeans(n_clusters = 4)\n",
    "        k_fit = kmeans.fit(df)\n",
    "        kumeler = k_fit.labels_\n",
    "        plt.scatter(df.iloc[:,0], df.iloc[:,1], c = kumeler, s = 50, cmap = \"viridis\")\n",
    "        merkezler = k_fit.cluster_centers_\n",
    "        plt.scatter(merkezler[:,0], merkezler[:,1], c = \"black\", s = 200, alpha = 0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataInfo(\"hmelq.csv\") #please enter the filename here for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dfdropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = Visualize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = Models(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.split(\"bad\", ['bad', 'reason', \"job\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.LogRegPrediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.dectreeclassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.knnclassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.lgbm_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.naivebayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.nnclassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.rfclassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.svmclass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataInfo(\"HW_Data_Set.xlsx\") #please enter filename here for regression\n",
    "df = df.dfdropna()\n",
    "df = df.replace(\"?\", np.mean)\n",
    "vis = Visualize(df)\n",
    "models = Models(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.split(\"50_target\", ['ind_109', 'ind_420', 'ind_422'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models.dectreeregressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.knnregressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.lgbm_regressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.nnregressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.rfregressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.svmregressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.kelbow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.cluster()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
